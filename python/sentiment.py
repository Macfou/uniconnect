# -*- coding: utf-8 -*-
"""sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1no02LKluk-_AV1hiZbRkdfp0GRNLUIzj
"""

!pip install nltk pandas

import nltk
import pandas as pd
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Download the necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Load dataset
data = pd.read_csv('sentiment_dataset_english_100k.csv')

# Check the first few rows of the dataset
print(data.head())

# Function to clean and preprocess text
def preprocess_text(text):
    # Tokenize text (convert to lowercase and tokenize)
    tokens = word_tokenize(text.lower())

    # Remove punctuation and stopwords
    tokens = [word for word in tokens if word not in string.punctuation and word not in stopwords.words('english')]

    return " ".join(tokens)

import re
import pandas as pd
from nltk.corpus import stopwords
import string

# Load your data
data = pd.read_csv('sentiment_dataset_english_100k.csv')

# Fallback tokenization using regex to avoid nltk issues
def preprocess_text_fallback(text):
    # Convert text to lowercase
    text = text.lower()

    # Remove punctuation
    text = ''.join([char for char in text if char not in string.punctuation])

    # Tokenize using regex
    words = re.findall(r'\b\w+\b', text)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    return " ".join(words)

# Apply the fallback preprocessing function
data['cleaned_feedback'] = data['Feedback'].apply(preprocess_text_fallback)

# Check the cleaned data
print(data[['Feedback', 'cleaned_feedback']].head())

from nltk.sentiment import SentimentIntensityAnalyzer

# Initialize SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()

# Function to classify sentiment (positive, neutral, negative)
def classify_sentiment(text):
    score = sia.polarity_scores(text)
    if score['compound'] >= 0.05:
        return 1  # Positive sentiment
    elif score['compound'] <= -0.05:
        return -1  # Negative sentiment
    else:
        return 0  # Neutral sentiment

# Apply sentiment classification to the cleaned feedback
data['sentiment'] = data['cleaned_feedback'].apply(classify_sentiment)

# Check the sentiment results
print(data[['Feedback', 'cleaned_feedback', 'sentiment']].head())

# Save the data with sentiments to a new CSV file
data.to_csv('sentiment_dataset_with_labels.csv', index=False)

print("Sentiment analysis completed and saved to 'sentiment_dataset_with_labels.csv'.")

pip install scikit-learn pandas nltk

import pandas as pd
from sklearn.model_selection import train_test_split

# Load your cleaned data (assuming the cleaned feedback and sentiment are ready)
data = pd.read_csv('sentiment_dataset_with_labels.csv')

# Features (X) and Labels (y)
X = data['cleaned_feedback']  # The cleaned feedback text
y = data['Sentiment']  # Sentiment labels (positive, negative, neutral)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training data size:", len(X_train))
print("Testing data size:", len(X_test))

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize the vectorizer
vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features based on your data size

# Fit the vectorizer to the training data and transform both training and testing data
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

print("TF-IDF matrix shape for training data:", X_train_tfidf.shape)

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score

# Initialize the Naive Bayes classifier
classifier = MultinomialNB()

# Train the classifier
classifier.fit(X_train_tfidf, y_train)

# Predict on the test data
y_pred = classifier.predict(X_test_tfidf)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Print classification report
print(classification_report(y_test, y_pred))

"""Check Class Distribution: Ensure the dataset has a balanced distribution of classes (-1, 0, 1)."""